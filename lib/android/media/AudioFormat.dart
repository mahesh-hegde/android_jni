// Autogenerated by jnigen. DO NOT EDIT!

// ignore_for_file: camel_case_types
// ignore_for_file: file_names
// ignore_for_file: non_constant_identifier_names
// ignore_for_file: constant_identifier_names
// ignore_for_file: unused_shown_name
// ignore_for_file: annotate_overrides
// ignore_for_file: no_leading_underscores_for_local_identifiers
// ignore_for_file: unused_import
// ignore_for_file: unused_element
// ignore_for_file: unused_field

import "package:jni/jni.dart" as jni;

import "package:jni/internal_helpers_for_jnigen.dart";

import "../os/Parcelable.dart" as parcelable_;

import "../os/Parcel.dart" as parcel_;
import "../../_init.dart" show jniEnv, jniAccessors;

/// from: android.media.AudioFormat
///
/// The AudioFormat class is used to access a number of audio format and
/// channel configuration constants. They are for instance used
/// in AudioTrack and AudioRecord, as valid values in individual parameters of
/// constructors like AudioTrack\#AudioTrack(int, int, int, int, int, int), where the fourth
/// parameter is one of the <code>AudioFormat.ENCODING_*</code> constants.
/// The <code>AudioFormat</code> constants are also used in MediaFormat to specify
/// audio related values commonly used in media, such as for MediaFormat\#KEY_CHANNEL_MASK.
/// The AudioFormat.Builder class can be used to create instances of
/// the <code>AudioFormat</code> format class.
/// Refer to
/// AudioFormat.Builder for documentation on the mechanics of the configuration and building
/// of such instances. Here we describe the main concepts that the <code>AudioFormat</code> class
/// allow you to convey in each instance, they are:
/// <ol>
/// <li><a href="\#sampleRate">sample rate</a>
/// <li><a href="\#encoding">encoding</a>
/// <li><a href="\#channelMask">channel masks</a>
/// </ol>
/// Closely associated with the <code>AudioFormat</code> is the notion of an
/// <a href="\#audioFrame">audio frame</a>, which is used throughout the documentation
/// to represent the minimum size complete unit of audio data.
///
/// <h4 id="sampleRate">Sample rate</h4>
/// Expressed in Hz, the sample rate in an <code>AudioFormat</code> instance expresses the number
/// of audio samples for each channel per second in the content you are playing or recording. It is
/// not the sample rate
/// at which content is rendered or produced. For instance a sound at a media sample rate of 8000Hz
/// can be played on a device operating at a sample rate of 48000Hz; the sample rate conversion is
/// automatically handled by the platform, it will not play at 6x speed.
///
/// As of API android.os.Build.VERSION_CODES\#M,
/// sample rates up to 192kHz are supported
/// for <code>AudioRecord</code> and <code>AudioTrack</code>, with sample rate conversion
/// performed as needed.
/// To improve efficiency and avoid lossy conversions, it is recommended to match the sample rate
/// for <code>AudioRecord</code> and <code>AudioTrack</code> to the endpoint device
/// sample rate, and limit the sample rate to no more than 48kHz unless there are special
/// device capabilities that warrant a higher rate.
///
/// <h4 id="encoding">Encoding</h4>
/// Audio encoding is used to describe the bit representation of audio data, which can be
/// either linear PCM or compressed audio, such as AC3 or DTS.
/// For linear PCM, the audio encoding describes the sample size, 8 bits, 16 bits, or 32 bits,
/// and the sample representation, integer or float.
/// <ul>
/// <li> \#ENCODING_PCM_8BIT: The audio sample is a 8 bit unsigned integer in the
/// range [0, 255], with a 128 offset for zero. This is typically stored as a Java byte in a
/// byte array or ByteBuffer. Since the Java byte is _signed_,
/// be careful with math operations and conversions as the most significant bit is inverted.
/// </li>
/// <li> \#ENCODING_PCM_16BIT: The audio sample is a 16 bit signed integer
/// typically stored as a Java short in a short array, but when the short
/// is stored in a ByteBuffer, it is native endian (as compared to the default Java big endian).
/// The short has full range from [-32768, 32767],
/// and is sometimes interpreted as fixed point Q.15 data.
/// </li>
/// <li> \#ENCODING_PCM_FLOAT: Introduced in
/// API android.os.Build.VERSION_CODES\#LOLLIPOP, this encoding specifies that
/// the audio sample is a 32 bit IEEE single precision float. The sample can be
/// manipulated as a Java float in a float array, though within a ByteBuffer
/// it is stored in native endian byte order.
/// The nominal range of <code>ENCODING_PCM_FLOAT</code> audio data is [-1.0, 1.0].
/// It is implementation dependent whether the positive maximum of 1.0 is included
/// in the interval. Values outside of the nominal range are clamped before
/// sending to the endpoint device. Beware that
/// the handling of NaN is undefined; subnormals may be treated as zero; and
/// infinities are generally clamped just like other values for <code>AudioTrack</code>
/// &ndash; try to avoid infinities because they can easily generate a NaN.
/// <br>
/// To achieve higher audio bit depth than a signed 16 bit integer short,
/// it is recommended to use <code>ENCODING_PCM_FLOAT</code> for audio capture, processing,
/// and playback.
/// Floats are efficiently manipulated by modern CPUs,
/// have greater precision than 24 bit signed integers,
/// and have greater dynamic range than 32 bit signed integers.
/// <code>AudioRecord</code> as of API android.os.Build.VERSION_CODES\#M and
/// <code>AudioTrack</code> as of API android.os.Build.VERSION_CODES\#LOLLIPOP
/// support <code>ENCODING_PCM_FLOAT</code>.
/// </li>
/// </ul>
/// For compressed audio, the encoding specifies the method of compression,
/// for example \#ENCODING_AC3 and \#ENCODING_DTS. The compressed
/// audio data is typically stored as bytes in
/// a byte array or ByteBuffer. When a compressed audio encoding is specified
/// for an <code>AudioTrack</code>, it creates a direct (non-mixed) track
/// for output to an endpoint (such as HDMI) capable of decoding the compressed audio.
/// For (most) other endpoints, which are not capable of decoding such compressed audio,
/// you will need to decode the data first, typically by creating a MediaCodec.
/// Alternatively, one may use MediaPlayer for playback of compressed
/// audio files or streams.
/// When compressed audio is sent out through a direct <code>AudioTrack</code>,
/// it need not be written in exact multiples of the audio access unit;
/// this differs from <code>MediaCodec</code> input buffers.
///
/// <h4 id="channelMask">Channel mask</h4>
/// Channel masks are used in <code>AudioTrack</code> and <code>AudioRecord</code> to describe
/// the samples and their arrangement in the audio frame. They are also used in the endpoint (e.g.
/// a USB audio interface, a DAC connected to headphones) to specify allowable configurations of a
/// particular device.
/// <br>As of API android.os.Build.VERSION_CODES\#M, there are two types of channel masks:
/// channel position masks and channel index masks.
///
/// <h5 id="channelPositionMask">Channel position masks</h5>
/// Channel position masks are the original Android channel masks, and are used since API
/// android.os.Build.VERSION_CODES\#BASE.
/// For input and output, they imply a positional nature - the location of a speaker or a microphone
/// for recording or playback.
/// <br>For a channel position mask, each allowed channel position corresponds to a bit in the
/// channel mask. If that channel position is present in the audio frame, that bit is set,
/// otherwise it is zero. The order of the bits (from lsb to msb) corresponds to the order of that
/// position's sample in the audio frame.
/// <br>The canonical channel position masks by channel count are as follows:
/// <br><table>
/// <tr><td>channel count</td><td>channel position mask</td></tr>
/// <tr><td>1</td><td>\#CHANNEL_OUT_MONO</td></tr>
/// <tr><td>2</td><td>\#CHANNEL_OUT_STEREO</td></tr>
/// <tr><td>3</td><td>\#CHANNEL_OUT_STEREO | \#CHANNEL_OUT_FRONT_CENTER</td></tr>
/// <tr><td>4</td><td>\#CHANNEL_OUT_QUAD</td></tr>
/// <tr><td>5</td><td>\#CHANNEL_OUT_QUAD | \#CHANNEL_OUT_FRONT_CENTER</td></tr>
/// <tr><td>6</td><td>\#CHANNEL_OUT_5POINT1</td></tr>
/// <tr><td>7</td><td>\#CHANNEL_OUT_5POINT1 | \#CHANNEL_OUT_BACK_CENTER</td></tr>
/// <tr><td>8</td><td>\#CHANNEL_OUT_7POINT1_SURROUND</td></tr>
/// </table>
/// <br>These masks are an ORed composite of individual channel masks. For example
/// \#CHANNEL_OUT_STEREO is composed of \#CHANNEL_OUT_FRONT_LEFT and
/// \#CHANNEL_OUT_FRONT_RIGHT.
///
/// <h5 id="channelIndexMask">Channel index masks</h5>
/// Channel index masks are introduced in API android.os.Build.VERSION_CODES\#M. They allow
/// the selection of a particular channel from the source or sink endpoint by number, i.e. the first
/// channel, the second channel, and so forth. This avoids problems with artificially assigning
/// positions to channels of an endpoint, or figuring what the i<sup>th</sup> position bit is within
/// an endpoint's channel position mask etc.
/// <br>Here's an example where channel index masks address this confusion: dealing with a 4 channel
/// USB device. Using a position mask, and based on the channel count, this would be a
/// \#CHANNEL_OUT_QUAD device, but really one is only interested in channel 0
/// through channel 3. The USB device would then have the following individual bit channel masks:
/// \#CHANNEL_OUT_FRONT_LEFT,
/// \#CHANNEL_OUT_FRONT_RIGHT, \#CHANNEL_OUT_BACK_LEFT
/// and \#CHANNEL_OUT_BACK_RIGHT. But which is channel 0 and which is
/// channel 3?
/// <br>For a channel index mask, each channel number is represented as a bit in the mask, from the
/// lsb (channel 0) upwards to the msb, numerically this bit value is
/// <code>1 << channelNumber</code>.
/// A set bit indicates that channel is present in the audio frame, otherwise it is cleared.
/// The order of the bits also correspond to that channel number's sample order in the audio frame.
/// <br>For the previous 4 channel USB device example, the device would have a channel index mask
/// <code>0xF</code>. Suppose we wanted to select only the first and the third channels; this would
/// correspond to a channel index mask <code>0x5</code> (the first and third bits set). If an
/// <code>AudioTrack</code> uses this channel index mask, the audio frame would consist of two
/// samples, the first sample of each frame routed to channel 0, and the second sample of each frame
/// routed to channel 2.
/// The canonical channel index masks by channel count are given by the formula
/// <code>(1 << channelCount) - 1</code>.
///
/// <h5>Use cases</h5>
/// <ul>
/// <li><i>Channel position mask for an endpoint:</i> <code>CHANNEL_OUT_FRONT_LEFT</code>,
///  <code>CHANNEL_OUT_FRONT_CENTER</code>, etc. for HDMI home theater purposes.
/// <li><i>Channel position mask for an audio stream:</i> Creating an <code>AudioTrack</code>
///  to output movie content, where 5.1 multichannel output is to be written.
/// <li><i>Channel index mask for an endpoint:</i> USB devices for which input and output do not
///  correspond to left or right speaker or microphone.
/// <li><i>Channel index mask for an audio stream:</i> An <code>AudioRecord</code> may only want the
///  third and fourth audio channels of the endpoint (i.e. the second channel pair), and not care the
///  about position it corresponds to, in which case the channel index mask is <code>0xC</code>.
///  Multichannel <code>AudioRecord</code> sessions should use channel index masks.
/// </ul>
/// <h4 id="audioFrame">Audio Frame</h4>
/// For linear PCM, an audio frame consists of a set of samples captured at the same time,
/// whose count and
/// channel association are given by the <a href="\#channelMask">channel mask</a>,
/// and whose sample contents are specified by the <a href="\#encoding">encoding</a>.
/// For example, a stereo 16 bit PCM frame consists of
/// two 16 bit linear PCM samples, with a frame size of 4 bytes.
/// For compressed audio, an audio frame may alternately
/// refer to an access unit of compressed data bytes that is logically grouped together for
/// decoding and bitstream access (e.g. MediaCodec),
/// or a single byte of compressed data (e.g. AudioTrack\#getBufferSizeInFrames() AudioTrack.getBufferSizeInFrames()),
/// or the linear PCM frame result from decoding the compressed data
/// (e.g.AudioTrack\#getPlaybackHeadPosition() AudioTrack.getPlaybackHeadPosition()),
/// depending on the context where audio frame is used.
class AudioFormat extends jni.JniObject {
  static final _classRef = jniAccessors.getClassOf("android/media/AudioFormat");
  AudioFormat.fromRef(jni.JObject ref) : super.fromRef(ref);

  /// from: static public final int CHANNEL_CONFIGURATION_DEFAULT
  ///
  /// @deprecated Use \#CHANNEL_OUT_DEFAULT or \#CHANNEL_IN_DEFAULT instead.
  static const CHANNEL_CONFIGURATION_DEFAULT = 1;

  /// from: static public final int CHANNEL_CONFIGURATION_INVALID
  ///
  /// @deprecated Use \#CHANNEL_INVALID instead.
  static const CHANNEL_CONFIGURATION_INVALID = 0;

  /// from: static public final int CHANNEL_CONFIGURATION_MONO
  ///
  /// @deprecated Use \#CHANNEL_OUT_MONO or \#CHANNEL_IN_MONO instead.
  static const CHANNEL_CONFIGURATION_MONO = 2;

  /// from: static public final int CHANNEL_CONFIGURATION_STEREO
  ///
  /// @deprecated Use \#CHANNEL_OUT_STEREO or \#CHANNEL_IN_STEREO instead.
  static const CHANNEL_CONFIGURATION_STEREO = 3;

  /// from: static public final int CHANNEL_INVALID
  ///
  /// Invalid audio channel mask
  static const CHANNEL_INVALID = 0;

  /// from: static public final int CHANNEL_IN_BACK
  static const CHANNEL_IN_BACK = 32;

  /// from: static public final int CHANNEL_IN_BACK_PROCESSED
  static const CHANNEL_IN_BACK_PROCESSED = 512;

  /// from: static public final int CHANNEL_IN_DEFAULT
  static const CHANNEL_IN_DEFAULT = 1;

  /// from: static public final int CHANNEL_IN_FRONT
  static const CHANNEL_IN_FRONT = 16;

  /// from: static public final int CHANNEL_IN_FRONT_PROCESSED
  static const CHANNEL_IN_FRONT_PROCESSED = 256;

  /// from: static public final int CHANNEL_IN_LEFT
  static const CHANNEL_IN_LEFT = 4;

  /// from: static public final int CHANNEL_IN_LEFT_PROCESSED
  static const CHANNEL_IN_LEFT_PROCESSED = 64;

  /// from: static public final int CHANNEL_IN_MONO
  static const CHANNEL_IN_MONO = 16;

  /// from: static public final int CHANNEL_IN_PRESSURE
  static const CHANNEL_IN_PRESSURE = 1024;

  /// from: static public final int CHANNEL_IN_RIGHT
  static const CHANNEL_IN_RIGHT = 8;

  /// from: static public final int CHANNEL_IN_RIGHT_PROCESSED
  static const CHANNEL_IN_RIGHT_PROCESSED = 128;

  /// from: static public final int CHANNEL_IN_STEREO
  static const CHANNEL_IN_STEREO = 12;

  /// from: static public final int CHANNEL_IN_VOICE_DNLINK
  static const CHANNEL_IN_VOICE_DNLINK = 32768;

  /// from: static public final int CHANNEL_IN_VOICE_UPLINK
  static const CHANNEL_IN_VOICE_UPLINK = 16384;

  /// from: static public final int CHANNEL_IN_X_AXIS
  static const CHANNEL_IN_X_AXIS = 2048;

  /// from: static public final int CHANNEL_IN_Y_AXIS
  static const CHANNEL_IN_Y_AXIS = 4096;

  /// from: static public final int CHANNEL_IN_Z_AXIS
  static const CHANNEL_IN_Z_AXIS = 8192;

  /// from: static public final int CHANNEL_OUT_5POINT1
  static const CHANNEL_OUT_5POINT1 = 252;

  /// from: static public final int CHANNEL_OUT_7POINT1
  ///
  /// @deprecated Not the typical 7.1 surround configuration. Use \#CHANNEL_OUT_7POINT1_SURROUND instead.
  static const CHANNEL_OUT_7POINT1 = 1020;

  /// from: static public final int CHANNEL_OUT_7POINT1_SURROUND
  static const CHANNEL_OUT_7POINT1_SURROUND = 6396;

  /// from: static public final int CHANNEL_OUT_BACK_CENTER
  static const CHANNEL_OUT_BACK_CENTER = 1024;

  /// from: static public final int CHANNEL_OUT_BACK_LEFT
  static const CHANNEL_OUT_BACK_LEFT = 64;

  /// from: static public final int CHANNEL_OUT_BACK_RIGHT
  static const CHANNEL_OUT_BACK_RIGHT = 128;

  /// from: static public final int CHANNEL_OUT_DEFAULT
  ///
  /// Default audio channel mask
  static const CHANNEL_OUT_DEFAULT = 1;

  /// from: static public final int CHANNEL_OUT_FRONT_CENTER
  static const CHANNEL_OUT_FRONT_CENTER = 16;

  /// from: static public final int CHANNEL_OUT_FRONT_LEFT
  static const CHANNEL_OUT_FRONT_LEFT = 4;

  /// from: static public final int CHANNEL_OUT_FRONT_LEFT_OF_CENTER
  static const CHANNEL_OUT_FRONT_LEFT_OF_CENTER = 256;

  /// from: static public final int CHANNEL_OUT_FRONT_RIGHT
  static const CHANNEL_OUT_FRONT_RIGHT = 8;

  /// from: static public final int CHANNEL_OUT_FRONT_RIGHT_OF_CENTER
  static const CHANNEL_OUT_FRONT_RIGHT_OF_CENTER = 512;

  /// from: static public final int CHANNEL_OUT_LOW_FREQUENCY
  static const CHANNEL_OUT_LOW_FREQUENCY = 32;

  /// from: static public final int CHANNEL_OUT_MONO
  static const CHANNEL_OUT_MONO = 4;

  /// from: static public final int CHANNEL_OUT_QUAD
  static const CHANNEL_OUT_QUAD = 204;

  /// from: static public final int CHANNEL_OUT_SIDE_LEFT
  static const CHANNEL_OUT_SIDE_LEFT = 2048;

  /// from: static public final int CHANNEL_OUT_SIDE_RIGHT
  static const CHANNEL_OUT_SIDE_RIGHT = 4096;

  /// from: static public final int CHANNEL_OUT_STEREO
  static const CHANNEL_OUT_STEREO = 12;

  /// from: static public final int CHANNEL_OUT_SURROUND
  static const CHANNEL_OUT_SURROUND = 1052;

  static final _id_CREATOR = jniAccessors.getStaticFieldIDOf(
      _classRef, "CREATOR", "Landroid/os/Parcelable\$Creator;");

  /// from: static public final android.os.Parcelable.Creator<android.media.AudioFormat> CREATOR
  /// The returned object must be deleted after use, by calling the `delete` method.
  static parcelable_.Parcelable_Creator get CREATOR =>
      parcelable_.Parcelable_Creator.fromRef(jniAccessors
          .getStaticField(_classRef, _id_CREATOR, jni.JniType.objectType)
          .object);

  /// from: static public final int ENCODING_AAC_ELD
  ///
  /// Audio data format: AAC ELD compressed
  static const ENCODING_AAC_ELD = 15;

  /// from: static public final int ENCODING_AAC_HE_V1
  ///
  /// Audio data format: AAC HE V1 compressed
  static const ENCODING_AAC_HE_V1 = 11;

  /// from: static public final int ENCODING_AAC_HE_V2
  ///
  /// Audio data format: AAC HE V2 compressed
  static const ENCODING_AAC_HE_V2 = 12;

  /// from: static public final int ENCODING_AAC_LC
  ///
  /// Audio data format: AAC LC compressed
  static const ENCODING_AAC_LC = 10;

  /// from: static public final int ENCODING_AAC_XHE
  ///
  /// Audio data format: AAC xHE compressed
  static const ENCODING_AAC_XHE = 16;

  /// from: static public final int ENCODING_AC3
  ///
  /// Audio data format: AC-3 compressed
  static const ENCODING_AC3 = 5;

  /// from: static public final int ENCODING_AC4
  ///
  /// Audio data format: AC-4 sync frame transport format
  static const ENCODING_AC4 = 17;

  /// from: static public final int ENCODING_DEFAULT
  ///
  /// Default audio data format
  static const ENCODING_DEFAULT = 1;

  /// from: static public final int ENCODING_DOLBY_TRUEHD
  ///
  /// Audio data format: DOLBY TRUEHD compressed
  static const ENCODING_DOLBY_TRUEHD = 14;

  /// from: static public final int ENCODING_DTS
  ///
  /// Audio data format: DTS compressed
  static const ENCODING_DTS = 7;

  /// from: static public final int ENCODING_DTS_HD
  ///
  /// Audio data format: DTS HD compressed
  static const ENCODING_DTS_HD = 8;

  /// from: static public final int ENCODING_E_AC3
  ///
  /// Audio data format: E-AC-3 compressed
  static const ENCODING_E_AC3 = 6;

  /// from: static public final int ENCODING_E_AC3_JOC
  ///
  /// Audio data format: E-AC-3-JOC compressed
  /// E-AC-3-JOC streams can be decoded by downstream devices supporting \#ENCODING_E_AC3.
  /// Use \#ENCODING_E_AC3 as the AudioTrack encoding when the downstream device
  /// supports \#ENCODING_E_AC3 but not \#ENCODING_E_AC3_JOC.
  static const ENCODING_E_AC3_JOC = 18;

  /// from: static public final int ENCODING_IEC61937
  ///
  /// Audio data format: compressed audio wrapped in PCM for HDMI
  /// or S/PDIF passthrough.
  /// IEC61937 uses a stereo stream of 16-bit samples as the wrapper.
  /// So the channel mask for the track must be \#CHANNEL_OUT_STEREO.
  /// Data should be written to the stream in a short[] array.
  /// If the data is written in a byte[] array then there may be endian problems
  /// on some platforms when converting to short internally.
  static const ENCODING_IEC61937 = 13;

  /// from: static public final int ENCODING_INVALID
  ///
  /// Invalid audio data format
  static const ENCODING_INVALID = 0;

  /// from: static public final int ENCODING_MP3
  ///
  /// Audio data format: MP3 compressed
  static const ENCODING_MP3 = 9;

  /// from: static public final int ENCODING_PCM_16BIT
  ///
  /// Audio data format: PCM 16 bit per sample. Guaranteed to be supported by devices.
  static const ENCODING_PCM_16BIT = 2;

  /// from: static public final int ENCODING_PCM_8BIT
  ///
  /// Audio data format: PCM 8 bit per sample. Not guaranteed to be supported by devices.
  static const ENCODING_PCM_8BIT = 3;

  /// from: static public final int ENCODING_PCM_FLOAT
  ///
  /// Audio data format: single-precision floating-point per sample
  static const ENCODING_PCM_FLOAT = 4;

  /// from: static public final int SAMPLE_RATE_UNSPECIFIED
  ///
  /// Sample rate will be a route-dependent value.
  /// For AudioTrack, it is usually the sink sample rate,
  /// and for AudioRecord it is usually the source sample rate.
  static const SAMPLE_RATE_UNSPECIFIED = 0;

  static final _id_ctor =
      jniAccessors.getMethodIDOf(_classRef, "<init>", "()V");

  /// from: void <init>()
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// @removed
  AudioFormat()
      : super.fromRef(
            jniAccessors.newObjectWithArgs(_classRef, _id_ctor, []).object);

  static final _id_getEncoding =
      jniAccessors.getMethodIDOf(_classRef, "getEncoding", "()I");

  /// from: public int getEncoding()
  ///
  /// Return the encoding.
  /// See the section on <a href="\#encoding">encodings</a> for more information about the different
  /// types of supported audio encoding.
  ///@return one of the values that can be set in Builder\#setEncoding(int) or
  /// AudioFormat\#ENCODING_INVALID if not set.
  int getEncoding() => jniAccessors.callMethodWithArgs(
      reference, _id_getEncoding, jni.JniType.intType, []).integer;

  static final _id_getSampleRate =
      jniAccessors.getMethodIDOf(_classRef, "getSampleRate", "()I");

  /// from: public int getSampleRate()
  ///
  /// Return the sample rate.
  ///@return one of the values that can be set in Builder\#setSampleRate(int) or
  /// \#SAMPLE_RATE_UNSPECIFIED if not set.
  int getSampleRate() => jniAccessors.callMethodWithArgs(
      reference, _id_getSampleRate, jni.JniType.intType, []).integer;

  static final _id_getChannelMask =
      jniAccessors.getMethodIDOf(_classRef, "getChannelMask", "()I");

  /// from: public int getChannelMask()
  ///
  /// Return the channel mask.
  /// See the section on <a href="\#channelMask">channel masks</a> for more information about
  /// the difference between index-based masks(as returned by \#getChannelIndexMask()) and
  /// the position-based mask returned by this function.
  ///@return one of the values that can be set in Builder\#setChannelMask(int) or
  /// AudioFormat\#CHANNEL_INVALID if not set.
  int getChannelMask() => jniAccessors.callMethodWithArgs(
      reference, _id_getChannelMask, jni.JniType.intType, []).integer;

  static final _id_getChannelIndexMask =
      jniAccessors.getMethodIDOf(_classRef, "getChannelIndexMask", "()I");

  /// from: public int getChannelIndexMask()
  ///
  /// Return the channel index mask.
  /// See the section on <a href="\#channelMask">channel masks</a> for more information about
  /// the difference between index-based masks, and position-based masks (as returned
  /// by \#getChannelMask()).
  ///@return one of the values that can be set in Builder\#setChannelIndexMask(int) or
  /// AudioFormat\#CHANNEL_INVALID if not set or an invalid mask was used.
  int getChannelIndexMask() => jniAccessors.callMethodWithArgs(
      reference, _id_getChannelIndexMask, jni.JniType.intType, []).integer;

  static final _id_getChannelCount =
      jniAccessors.getMethodIDOf(_classRef, "getChannelCount", "()I");

  /// from: public int getChannelCount()
  ///
  /// Return the channel count.
  ///@return the channel count derived from the channel position mask or the channel index mask.
  /// Zero is returned if both the channel position mask and the channel index mask are not set.
  int getChannelCount() => jniAccessors.callMethodWithArgs(
      reference, _id_getChannelCount, jni.JniType.intType, []).integer;

  static final _id_equals1 =
      jniAccessors.getMethodIDOf(_classRef, "equals", "(Ljava/lang/Object;)Z");

  /// from: public boolean equals(java.lang.Object o)
  bool equals1(jni.JniObject o) => jniAccessors.callMethodWithArgs(
      reference, _id_equals1, jni.JniType.booleanType, [o.reference]).boolean;

  static final _id_hashCode1 =
      jniAccessors.getMethodIDOf(_classRef, "hashCode", "()I");

  /// from: public int hashCode()
  int hashCode1() => jniAccessors.callMethodWithArgs(
      reference, _id_hashCode1, jni.JniType.intType, []).integer;

  static final _id_describeContents =
      jniAccessors.getMethodIDOf(_classRef, "describeContents", "()I");

  /// from: public int describeContents()
  int describeContents() => jniAccessors.callMethodWithArgs(
      reference, _id_describeContents, jni.JniType.intType, []).integer;

  static final _id_writeToParcel = jniAccessors.getMethodIDOf(
      _classRef, "writeToParcel", "(Landroid/os/Parcel;I)V");

  /// from: public void writeToParcel(android.os.Parcel dest, int flags)
  void writeToParcel(parcel_.Parcel dest, int flags) =>
      jniAccessors.callMethodWithArgs(reference, _id_writeToParcel,
          jni.JniType.voidType, [dest.reference, flags]).check();

  static final _id_toString1 =
      jniAccessors.getMethodIDOf(_classRef, "toString", "()Ljava/lang/String;");

  /// from: public java.lang.String toString()
  /// The returned object must be deleted after use, by calling the `delete` method.
  jni.JniString toString1() =>
      jni.JniString.fromRef(jniAccessors.callMethodWithArgs(
          reference, _id_toString1, jni.JniType.objectType, []).object);
}

/// from: android.media.AudioFormat$Builder
///
/// Builder class for AudioFormat objects.
/// Use this class to configure and create an AudioFormat instance. By setting format
/// characteristics such as audio encoding, channel mask or sample rate, you indicate which
/// of those are to vary from the default behavior on this device wherever this audio format
/// is used. See AudioFormat for a complete description of the different parameters that
/// can be used to configure an <code>AudioFormat</code> instance.
/// AudioFormat is for instance used in
/// AudioTrack\#AudioTrack(AudioAttributes, AudioFormat, int, int, int). In this
/// constructor, every format characteristic set on the <code>Builder</code> (e.g. with
/// \#setSampleRate(int)) will alter the default values used by an
/// <code>AudioTrack</code>. In this case for audio playback with <code>AudioTrack</code>, the
/// sample rate set in the <code>Builder</code> would override the platform output sample rate
/// which would otherwise be selected by default.
class AudioFormat_Builder extends jni.JniObject {
  static final _classRef =
      jniAccessors.getClassOf("android/media/AudioFormat\$Builder");
  AudioFormat_Builder.fromRef(jni.JObject ref) : super.fromRef(ref);

  static final _id_ctor =
      jniAccessors.getMethodIDOf(_classRef, "<init>", "()V");

  /// from: public void <init>()
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Constructs a new Builder with none of the format characteristics set.
  AudioFormat_Builder()
      : super.fromRef(
            jniAccessors.newObjectWithArgs(_classRef, _id_ctor, []).object);

  static final _id_ctor1 = jniAccessors.getMethodIDOf(
      _classRef, "<init>", "(Landroid/media/AudioFormat;)V");

  /// from: public void <init>(android.media.AudioFormat af)
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Constructs a new Builder from a given AudioFormat.
  ///@param af the AudioFormat object whose data will be reused in the new Builder.
  AudioFormat_Builder.ctor1(AudioFormat af)
      : super.fromRef(jniAccessors
            .newObjectWithArgs(_classRef, _id_ctor1, [af.reference]).object);

  static final _id_build = jniAccessors.getMethodIDOf(
      _classRef, "build", "()Landroid/media/AudioFormat;");

  /// from: public android.media.AudioFormat build()
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Combines all of the format characteristics that have been set and return a new
  /// AudioFormat object.
  ///@return a new AudioFormat object
  AudioFormat build() => AudioFormat.fromRef(jniAccessors.callMethodWithArgs(
      reference, _id_build, jni.JniType.objectType, []).object);

  static final _id_setEncoding = jniAccessors.getMethodIDOf(
      _classRef, "setEncoding", "(I)Landroid/media/AudioFormat\$Builder;");

  /// from: public android.media.AudioFormat.Builder setEncoding(int encoding)
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Sets the data encoding format.
  ///@param encoding the specified encoding or default.
  /// Value is android.media.AudioFormat\#ENCODING_DEFAULT, android.media.AudioFormat\#ENCODING_PCM_8BIT, android.media.AudioFormat\#ENCODING_PCM_16BIT, android.media.AudioFormat\#ENCODING_PCM_FLOAT, android.media.AudioFormat\#ENCODING_AC3, android.media.AudioFormat\#ENCODING_E_AC3, android.media.AudioFormat\#ENCODING_E_AC3_JOC, android.media.AudioFormat\#ENCODING_DTS, android.media.AudioFormat\#ENCODING_DTS_HD, android.media.AudioFormat\#ENCODING_IEC61937, android.media.AudioFormat\#ENCODING_AAC_HE_V1, android.media.AudioFormat\#ENCODING_AAC_HE_V2, android.media.AudioFormat\#ENCODING_AAC_LC, android.media.AudioFormat\#ENCODING_AAC_ELD, android.media.AudioFormat\#ENCODING_AAC_XHE, or android.media.AudioFormat\#ENCODING_AC4
  ///@return the same Builder instance.
  ///@throws java.lang.IllegalArgumentException
  AudioFormat_Builder setEncoding(int encoding) =>
      AudioFormat_Builder.fromRef(jniAccessors.callMethodWithArgs(reference,
          _id_setEncoding, jni.JniType.objectType, [encoding]).object);

  static final _id_setChannelMask = jniAccessors.getMethodIDOf(
      _classRef, "setChannelMask", "(I)Landroid/media/AudioFormat\$Builder;");

  /// from: public android.media.AudioFormat.Builder setChannelMask(int channelMask)
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Sets the channel position mask.
  /// The channel position mask specifies the association between audio samples in a frame
  /// with named endpoint channels. The samples in the frame correspond to the
  /// named set bits in the channel position mask, in ascending bit order.
  /// See \#setChannelIndexMask(int) to specify channels
  /// based on endpoint numbered channels. This <a href="\#channelPositionMask>description of
  /// channel position masks</a> covers the concept in more details.
  ///@param channelMask describes the configuration of the audio channels.
  ///     For output, the channelMask can be an OR-ed combination of
  ///    channel position masks, e.g.
  ///    AudioFormat\#CHANNEL_OUT_FRONT_LEFT,
  ///    AudioFormat\#CHANNEL_OUT_FRONT_RIGHT,
  ///    AudioFormat\#CHANNEL_OUT_FRONT_CENTER,
  ///    AudioFormat\#CHANNEL_OUT_LOW_FREQUENCY
  ///    AudioFormat\#CHANNEL_OUT_BACK_LEFT,
  ///    AudioFormat\#CHANNEL_OUT_BACK_RIGHT,
  ///    AudioFormat\#CHANNEL_OUT_BACK_CENTER,
  ///    AudioFormat\#CHANNEL_OUT_SIDE_LEFT,
  ///    AudioFormat\#CHANNEL_OUT_SIDE_RIGHT.
  ///     For a valid AudioTrack channel position mask,
  ///    the following conditions apply:
  ///    <br> (1) at most eight channel positions may be used;
  ///    <br> (2) right/left pairs should be matched.
  ///     For input or AudioRecord, the mask should be
  ///    AudioFormat\#CHANNEL_IN_MONO or
  ///    AudioFormat\#CHANNEL_IN_STEREO.  AudioFormat\#CHANNEL_IN_MONO is
  ///    guaranteed to work on all devices.
  ///@return the same <code>Builder</code> instance.
  /// This value will never be {@code null}.
  ///@throws IllegalArgumentException if the channel mask is invalid or
  ///    if both channel index mask and channel position mask
  ///    are specified but do not have the same channel count.
  AudioFormat_Builder setChannelMask(int channelMask) =>
      AudioFormat_Builder.fromRef(jniAccessors.callMethodWithArgs(reference,
          _id_setChannelMask, jni.JniType.objectType, [channelMask]).object);

  static final _id_setChannelIndexMask = jniAccessors.getMethodIDOf(_classRef,
      "setChannelIndexMask", "(I)Landroid/media/AudioFormat\$Builder;");

  /// from: public android.media.AudioFormat.Builder setChannelIndexMask(int channelIndexMask)
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Sets the channel index mask.
  /// A channel index mask specifies the association of audio samples in the frame
  /// with numbered endpoint channels. The i-th bit in the channel index
  /// mask corresponds to the i-th endpoint channel.
  /// For example, an endpoint with four channels is represented
  /// as index mask bits 0 through 3. This <a href="\#channelIndexMask>description of channel
  /// index masks</a> covers the concept in more details.
  /// See \#setChannelMask(int) for a positional mask interpretation.
  ///  Both AudioTrack and AudioRecord support
  /// a channel index mask.
  /// If a channel index mask is specified it is used,
  /// otherwise the channel position mask specified
  /// by <code>setChannelMask</code> is used.
  /// For <code>AudioTrack</code> and <code>AudioRecord</code>,
  /// a channel position mask is not required if a channel index mask is specified.
  ///@param channelIndexMask describes the configuration of the audio channels.
  ///     For output, the <code>channelIndexMask</code> is an OR-ed combination of
  ///    bits representing the mapping of <code>AudioTrack</code> write samples
  ///    to output sink channels.
  ///    For example, a mask of <code>0xa</code>, or binary <code>1010</code>,
  ///    means the <code>AudioTrack</code> write frame consists of two samples,
  ///    which are routed to the second and the fourth channels of the output sink.
  ///    Unmatched output sink channels are zero filled and unmatched
  ///    <code>AudioTrack</code> write samples are dropped.
  ///     For input, the <code>channelIndexMask</code> is an OR-ed combination of
  ///    bits representing the mapping of input source channels to
  ///    <code>AudioRecord</code> read samples.
  ///    For example, a mask of <code>0x5</code>, or binary
  ///    <code>101</code>, will read from the first and third channel of the input
  ///    source device and store them in the first and second sample of the
  ///    <code>AudioRecord</code> read frame.
  ///    Unmatched input source channels are dropped and
  ///    unmatched <code>AudioRecord</code> read samples are zero filled.
  ///@return the same <code>Builder</code> instance.
  /// This value will never be {@code null}.
  ///@throws IllegalArgumentException if the channel index mask is invalid or
  ///    if both channel index mask and channel position mask
  ///    are specified but do not have the same channel count.
  AudioFormat_Builder setChannelIndexMask(int channelIndexMask) =>
      AudioFormat_Builder.fromRef(jniAccessors.callMethodWithArgs(
          reference,
          _id_setChannelIndexMask,
          jni.JniType.objectType,
          [channelIndexMask]).object);

  static final _id_setSampleRate = jniAccessors.getMethodIDOf(
      _classRef, "setSampleRate", "(I)Landroid/media/AudioFormat\$Builder;");

  /// from: public android.media.AudioFormat.Builder setSampleRate(int sampleRate)
  /// The returned object must be deleted after use, by calling the `delete` method.
  ///
  /// Sets the sample rate.
  ///@param sampleRate the sample rate expressed in Hz
  ///@return the same Builder instance.
  ///@throws java.lang.IllegalArgumentException
  AudioFormat_Builder setSampleRate(int sampleRate) =>
      AudioFormat_Builder.fromRef(jniAccessors.callMethodWithArgs(reference,
          _id_setSampleRate, jni.JniType.objectType, [sampleRate]).object);
}
